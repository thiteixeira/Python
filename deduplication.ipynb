{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deduplication.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thiteixeira/Python/blob/master/deduplication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rOND0WJDOP4S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "80e96fd6-ff9a-424a-def1-cfb38ec7e502"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import unittest\n",
        "import tempfile\n",
        "import hashlib\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "\n",
        "#original = 'original.data'\n",
        "#deduped = 'deduped_file.data'\n",
        "#reduped = 'reduped_file.data'\n",
        "BLOCK_SIZE = 1024\n",
        "\n",
        "\n",
        "def dedup(inpath, outpath):\n",
        "    hashes = {}\n",
        "    index = 0\n",
        "\n",
        "    # read file block of 1 kB at a time\n",
        "    with open(inpath, 'rb') as f, open(outpath, \"wb\") as outFile:\n",
        "        while(True):\n",
        "            md5 = hashlib.md5()\n",
        "            chunk = f.read(BLOCK_SIZE)\n",
        "            if not chunk:\n",
        "                # we are done here, write outFile!\n",
        "                #print(hashes)\n",
        "                pickle.dump(hashes, outFile, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "                return\n",
        "\n",
        "            # hash each chunk\n",
        "            md5.update(chunk)\n",
        "            chunk_id = (md5.hexdigest())\n",
        "\n",
        "            duplicates = hashes.get(chunk_id)\n",
        "\n",
        "            # if current hash is already in the dictionary, update value\n",
        "            if (duplicates):\n",
        "                hashes[chunk_id].append(index)\n",
        "\n",
        "            else:\n",
        "                hashes[chunk_id] = []\n",
        "                hashes[chunk_id].append(chunk)\n",
        "                hashes[chunk_id].append(index)\n",
        "\n",
        "            index += 1\n",
        "\n",
        "def redup(inpath, outpath):\n",
        "    # read reduped binary file\n",
        "    result = ()\n",
        "    with open(inpath, 'rb') as f:#, open(outpath, \"wb\") as outFile:\n",
        "        bytes_list = []\n",
        "        a_dict = pickle.load(f)\n",
        "        for values_list in sorted(a_dict.values()):\n",
        "            #print(values_list)\n",
        "            file_bytes = values_list.pop(0)\n",
        "            for val in values_list:\n",
        "                bytes_list.insert(val, file_bytes)\n",
        "        #print(bytes_list)\n",
        "        result = tuple(bytes_list)\n",
        "        #print(result)\n",
        "    with open(outpath, 'wb') as f:\n",
        "        for chunk in result:\n",
        "            f.write(chunk)\n",
        "    return\n",
        "\n",
        "\n",
        "def gen_chunk():\n",
        "    return bytes(random.randrange(256) for _ in range(1024))\n",
        "\n",
        "def write_file(filepath, chunks):\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, 'wb') as f:\n",
        "        for chunk in chunks:\n",
        "            md5.update(chunk)\n",
        "            f.write(chunk)\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def read_file(filepath):\n",
        "    md5 = hashlib.md5()\n",
        "    with open(filepath, 'rb') as f:\n",
        "        md5.update(f.read())\n",
        "    return md5.hexdigest()\n",
        "\n",
        "def process(chunks):\n",
        "    print('Creating original file')\n",
        "    before = write_file(original, chunks)\n",
        "    print('Original hash: ' + str(before))\n",
        "    dedup(original, deduped)\n",
        "    size = os.path.getsize(original), os.path.getsize(deduped)\n",
        "    if (size[0] <= size[1]):\n",
        "        print(\"\\nAssertion failure -- Deduplicated file is larger than the original.\\n\")\n",
        "    print('\\nOriginal Size {} (bytes) -> Deduped Size {} (bytes)\\n'.format(size[0], size[1]))\n",
        "\n",
        "\n",
        "    redup(deduped, reduped)\n",
        "    after = read_file(reduped)\n",
        "    print('Reduped hashed: ' + str(after))\n",
        "    size = os.path.getsize(deduped), os.path.getsize(reduped)\n",
        "    print('\\nDeduped Size {} (bytes) -> Reduped Size {} (bytes)\\n'.format(size[0], size[1]))\n",
        "    if (before == after):\n",
        "        print('True')\n",
        "    else:\n",
        "        print(\"Assertion failure -- Reduplicated file is not the same as the original.\")\n",
        "\n",
        "\n",
        "# Note: the class must be called Test\n",
        "class Test(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        with tempfile.NamedTemporaryFile('wb', delete=False) as f:\n",
        "            self.original = f.name\n",
        "\n",
        "        with tempfile.NamedTemporaryFile('wb', delete=False) as f:\n",
        "            self.deduped = f.name\n",
        "\n",
        "        with tempfile.NamedTemporaryFile('wb', delete=False) as f:\n",
        "            self.reduped = f.name\n",
        "\n",
        "    def gen_chunk(self):\n",
        "        return bytes(random.randrange(256) for _ in range(1024))\n",
        "\n",
        "    def write_file(self, filepath, chunks):\n",
        "        md5 = hashlib.md5()\n",
        "        with open(filepath, 'wb') as f:\n",
        "            for chunk in chunks:\n",
        "                md5.update(chunk)\n",
        "                f.write(chunk)\n",
        "        return md5.hexdigest()\n",
        "\n",
        "    def read_file(self, filepath):\n",
        "        md5 = hashlib.md5()\n",
        "        with open(filepath, 'rb') as f:\n",
        "            md5.update(f.read())\n",
        "        return md5.hexdigest()\n",
        "\n",
        "    def process(self, chunks):\n",
        "        before = self.write_file(self.original, chunks)\n",
        "        dedup(self.original, self.deduped)\n",
        "\n",
        "        size = os.path.getsize(self.original), os.path.getsize(self.deduped)\n",
        "        self.assertTrue(size[0] >= size[1], \"Assertion failure -- Deduplicated file is larger than the original.\")\n",
        "        print('Original Size {} (bytes) -> Deduped Size {} (bytes)'.format(size[0], size[1]))\n",
        "\n",
        "        redup(self.deduped, self.reduped)\n",
        "        after = self.read_file(self.reduped)\n",
        "        self.assertEqual(before, after, \"Assertion failure -- Reduplicated file is not the same as the original.\")\n",
        "\n",
        "    def test_basic_test(self):\n",
        "        random.seed(0)\n",
        "        chunk = self.gen_chunk()\n",
        "        chunks = chunk, chunk, chunk\n",
        "        self.process(chunks)\n",
        "\n",
        "    def test_small_file(self):\n",
        "        random.seed(0)\n",
        "        pool = [self.gen_chunk() for _ in range(2)]\n",
        "        chunks = pool[0], pool[1], pool[0], pool[0], pool[1]\n",
        "        self.process(chunks)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".."
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Original Size 3072 (bytes) -> Deduped Size 1090 (bytes)\n",
            "Original Size 5120 (bytes) -> Deduped Size 2164 (bytes)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 2 tests in 0.012s\n",
            "\n",
            "OK\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}